{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f85585",
   "metadata": {},
   "source": [
    "# RESUME CATEGORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30be493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('UpdatedResumeDataSet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c668e6",
   "metadata": {},
   "source": [
    "Category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eece2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(df['Category'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts =  df['Category'].value_counts()\n",
    "label = df['Category'].unique()\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.pie(counts, labels=label, autopct='%1.1f%%', shadow=True, colors=plt.cm.plasma(np.linspace(0,1,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4afa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e1763",
   "metadata": {},
   "source": [
    "Cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resume']=df['Resume'].apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0239a99",
   "metadata": {},
   "source": [
    "words into categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00537576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "le.fit(df['Category'])\n",
    "df['Category'] = le.transform(df['Category'])\n",
    "pickle.dump(le, open('label_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa768a",
   "metadata": {},
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(df['Resume'])\n",
    "requiredText=tfidf.transform(df['Resume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a95f8c",
   "metadata": {},
   "source": [
    "splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(requiredText, df['Category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55d356",
   "metadata": {},
   "source": [
    "model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf= OneVsRestClassifier(KNeighborsClassifier())\n",
    "clf.fit(x_train, y_train)\n",
    "ypred=clf.predict(x_test)\n",
    "print(accuracy_score(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24755d54",
   "metadata": {},
   "source": [
    "prediction system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "pickle.dump(clf, open('clf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6aa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myresume = \"\"\"I am a data scientist specializing in machine\n",
    "learning, deep learning, and computer vision. With\n",
    "a strong background in mathematics, statistics,\n",
    "and programming, I am passionate about\n",
    "uncovering hidden patterns and insights in data.\n",
    "I have extensive experience in developing\n",
    "predictive models, implementing deep learning\n",
    "algorithms, and designing computer vision\n",
    "systems. My technical skills include proficiency in\n",
    "Python, Sklearn, TensorFlow, and PyTorch.\n",
    "What sets me apart is my ability to effectively\n",
    "communicate complex concepts to diverse\n",
    "audiences. I excel in translating technical insights\n",
    "into actionable recommendations that drive\n",
    "informed decision-making.\n",
    "If you're looking for a dedicated and versatile data\n",
    "scientist to collaborate on impactful projects, I am\n",
    "eager to contribute my expertise. Let's harness the\n",
    "power of data together to unlock new possibilities\n",
    "and shape a better future.\n",
    "Contact & Sources\n",
    "Email: 611noorsaeed@gmail.com\n",
    "Phone: 03442826192\n",
    "Github: https://github.com/611noorsaeed\n",
    "Linkdin: https://www.linkedin.com/in/noor-saeed654a23263/\n",
    "Blogs: https://medium.com/@611noorsaeed\n",
    "Youtube: Artificial Intelligence\n",
    "ABOUT ME\n",
    "WORK EXPERIENCE\n",
    "SKILLES\n",
    "NOOR SAEED\n",
    "LANGUAGES\n",
    "English\n",
    "Urdu\n",
    "Hindi\n",
    "I am a versatile data scientist with expertise in a wide\n",
    "range of projects, including machine learning,\n",
    "recommendation systems, deep learning, and computer\n",
    "vision. Throughout my career, I have successfully\n",
    "developed and deployed various machine learning models\n",
    "to solve complex problems and drive data-driven\n",
    "decision-making\n",
    "Machine Learnine\n",
    "Deep Learning\n",
    "Computer Vision\n",
    "Recommendation Systems\n",
    "Data Visualization\n",
    "Programming Languages (Python, SQL)\n",
    "Data Preprocessing and Feature Engineering\n",
    "Model Evaluation and Deployment\n",
    "Statistical Analysis\n",
    "Communication and Collaboration\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd72190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "clf=pickle.load(open('clf.pkl', 'rb'))\n",
    "cleaned_resume=cleanResume(myresume)\n",
    "input_features=tfidf.transform([cleaned_resume])\n",
    "\n",
    "prediction_id=clf.predict(input_features)[0]\n",
    "print(prediction_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13e43e",
   "metadata": {},
   "source": [
    "# RESUME ATS SCORE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b488bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d179730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample Entry Type: <class 'tuple'>\n",
      "üìÑ Sample Entry Content: ('Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month ‚Ä¢ Salesforce Developer Oracle 5 Years 2 Month ‚Ä¢ Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/', {'entities': [(1749, 1755, 'Companies worked at'), (1696, 1702, 'Companies worked at'), (1417, 1423, 'Companies worked at'), (1356, 1793, 'Skills'), (1209, 1215, 'Companies worked at'), (1136, 1248, 'Skills'), (928, 932, 'Graduation Year'), (858, 889, 'College Name'), (821, 856, 'Degree'), (787, 791, 'Graduation Year'), (744, 750, 'Companies worked at'), (722, 742, 'Designation'), (658, 664, 'Companies worked at'), (640, 656, 'Designation'), (574, 580, 'Companies worked at'), (555, 573, 'Designation'), (470, 493, 'Companies worked at'), (444, 469, 'Designation'), (308, 314, 'Companies worked at'), (234, 240, 'Companies worked at'), (175, 198, 'Companies worked at'), (93, 137, 'Email Address'), (39, 48, 'Location'), (13, 38, 'Designation'), (0, 12, 'Name')]})\n",
      "\n",
      "‚úÖ Loaded 200 resumes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3001ba1b01c40618c3b016ae355e3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b70ead2d664520a55978c4b2907754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8296fa5eb5a542febd7d29d6328d78f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d54719282af4e12be5339761dea9977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d79733ae8c488994616b914335e589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d0043ec05b407cacf474f68a961a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b581249c77f9460391f6a88c64893666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d775f38f85354550a779da4b13d77ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d11152abc914b0ebaa7086f8f3d139e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62792f4315284b5ca328d8865a7598e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4124288451b44819d651094ad352c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Calculating ATS Match Scores...\n",
      "\n",
      "üìÑ Resume 1\n",
      "   üîó JD 1 ATS Score: 29.04%\n",
      "   üîó JD 2 ATS Score: 13.53%\n",
      "----------------------------------------\n",
      "üìÑ Resume 2\n",
      "   üîó JD 1 ATS Score: 27.23%\n",
      "   üîó JD 2 ATS Score: 13.06%\n",
      "----------------------------------------\n",
      "üìÑ Resume 3\n",
      "   üîó JD 1 ATS Score: 35.58%\n",
      "   üîó JD 2 ATS Score: 19.05%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# === Load Kaggle ATS Dataset and Compute ATS Scores ===\n",
    "\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Step 1: Load the parsed resume data (as tuple list)\n",
    "with open('E:/FINAL_YEAR_PROJECT/Resume Screening/backend/ats/train_data.pkl', 'rb') as f:\n",
    "    kaggle_data = pickle.load(f)\n",
    "\n",
    "# Step 2: Check structure\n",
    "print(\"üìÑ Sample Entry Type:\", type(kaggle_data[0]))\n",
    "print(\"üìÑ Sample Entry Content:\", kaggle_data[0])\n",
    "\n",
    "# Step 3: Convert each tuple into resume text\n",
    "resume_texts = []\n",
    "for tup in kaggle_data:\n",
    "    # Safely extract each component\n",
    "    summary     = tup[0] if len(tup) > 0 else \"\"\n",
    "    skills      = tup[1] if len(tup) > 1 else \"\"\n",
    "    experience  = tup[2] if len(tup) > 2 else \"\"\n",
    "    designation = tup[3] if len(tup) > 3 else \"\"\n",
    "\n",
    "    # Format resume text\n",
    "    resume = f\"\"\"\n",
    "    Skills: {skills}\n",
    "    Summary: {summary}\n",
    "    Experience: {experience}\n",
    "    Designation: {designation}\n",
    "    \"\"\"\n",
    "    resume_texts.append(resume.strip())\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(resume_texts)} resumes.\")\n",
    "\n",
    "# Step 4: Define Sample Job Descriptions\n",
    "job_descriptions = [\n",
    "    \"We are looking for a data scientist skilled in Python, machine learning, and data visualization.\",\n",
    "    \"Frontend React developer needed with strong UI/UX skills and JavaScript expertise.\"\n",
    "]\n",
    "\n",
    "# Step 5: Load Semantic Similarity Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 6: Calculate and Display ATS Match Scores\n",
    "print(\"\\nüîç Calculating ATS Match Scores...\\n\")\n",
    "for i, resume in enumerate(resume_texts[:3]):  # Show only first 3 for demo\n",
    "    print(f\"üìÑ Resume {i+1}\")\n",
    "    for j, jd in enumerate(job_descriptions):\n",
    "        score = util.cos_sim(\n",
    "            model.encode(resume, convert_to_tensor=True),\n",
    "            model.encode(jd, convert_to_tensor=True)\n",
    "        )[0][0]\n",
    "        print(f\"   üîó JD {j+1} ATS Score: {round(float(score) * 100, 2)}%\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7279168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
